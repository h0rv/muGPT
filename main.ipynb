{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d eswarreddy12/family-guy-dialogues-with-various-lexicon-ratings\n",
    "# !mkdir -p data\n",
    "# !unzip family-guy-dialogues-with-various-lexicon-ratings.zip -d data\n",
    "# !rm family-guy-dialogues-with-various-lexicon-ratings.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Family_Guy_Final_NRC_AFINN_BING.csv   GPT_lex11.csv   Ratings_FG5.csv\n"
     ]
    }
   ],
   "source": [
    "!ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season, Episode, Time_Stamp, Dialogue, NRC_Sentiment, AFINN_Sentiment, AFINN_Sentiment_Score, BING_Sentiment\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = \"data/Family_Guy_Final_NRC_AFINN_BING.csv\"\n",
    "\n",
    "# Print headings\n",
    "with open(file_path, \"r\") as f:\n",
    "    csvf = csv.reader(f, delimiter=\",\", quotechar=\"'\")\n",
    "    headings = next(csvf)  # Read the first row (headings)\n",
    "    print(\", \".join(headings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "# Read dialogue and create the corpus\n",
    "with open(file_path, \"r\") as f:\n",
    "    csvf = csv.DictReader(f, delimiter=\",\", quotechar=\"'\")\n",
    "    for row in csvf:\n",
    "        dialogue = row[\"Dialogue\"].strip('\"')  # Remove double quotes\n",
    "        corpus.append(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\n",
      "Mom\n",
      "Greg\n",
      "No\n",
      "He's lying. There's no doubt about that.\n",
      "Greg\n",
      "Corpus line num: 154844\n",
      "Corpus str len: 3623337\n",
      "Mom\n",
      "Greg\n",
      "No\n",
      "He's lying. There's no doubt about that.\n",
      "Greg\n",
      "That'll give you time to think about what you did.\n",
      "Man!\n",
      "That'll teach him.\n",
      "Jan\n",
      "Smoking. How does a boy like that go so wrong?\n",
      "They live in a crummy neighborhood.\n",
      "The Bradys?\n",
      "Yeah. They got robbers\n",
      "You folks want some pancakes?\n",
      "No\n",
      "Mom\n",
      "Meg\n",
      "You know\n",
      "Excellent! The mind-control device is nearing completion!\n",
      "Stewie\n",
      "Damn you\n",
      "You've impeded my work since the day I escaped from your wretched womb.\n",
      "Don't pout\n",
      "But\n",
      "No toys\n",
      "Very well\n",
      "Mark my words\n",
      "Mom\n",
      "Don't touch the thermostat\n",
      "Come on. This thing goes up to 90.\n",
      "Who touched the thermostat?\n",
      "God\n",
      "Brain implant\n",
      "Tells you when the kids mess with the dial.\n",
      "My thing went off! Your thermostat okay?\n",
      "Yeah\n",
      "Is my kid over here?\n",
      "Forget it! False alarm!\n",
      "Ass ahoy.\n",
      "Peter\n",
      "He's going to a stag party.\n",
      "Lois\n",
      "I am the man of the house.\n",
      "As the man\n",
      "Look\n",
      "Come on. You're worrying about nothing.\n",
      "Remember when you got drunk off the Communion wine at church?\n",
      "And so the Lord God smote poor Job with festering boils all o\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 entries of the corpus\n",
    "print(\"Corpus:\")\n",
    "for i in range(5):\n",
    "    print(corpus[i])\n",
    "\n",
    "print(f\"Corpus line num: {len(corpus)}\")\n",
    "\n",
    "corpus_str = \"\\n\".join(corpus)\n",
    "\n",
    "print(f\"Corpus str len: {len(corpus_str)}\")\n",
    "\n",
    "print(corpus_str[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz{}¤ª³¶¿ÁÂÃÇÉÊÓàáâãçèéêíñóôúû‰™\n",
      "Vocab size: 122\n"
     ]
    }
   ],
   "source": [
    "# Unique chars in the corpus\n",
    "\n",
    "chars = sorted(list(set(corpus_str)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# TODO: would need to remove chars from corpus too\n",
    "# chars = chars[:91] # remove special chars\n",
    "# vocab_size = len(chars)\n",
    "# print(''.join(chars))\n",
    "# print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 73, 1, 84, 72, 69, 82, 69, 2]\n",
      "Hi there!\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from chars to ints\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"Hi there!\"))\n",
    "print(decode(encode(\"Hi there!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(corpus_str), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3623337])\n",
      "torch.int64\n",
      "tensor([46, 79, 77,  0, 40, 82, 69, 71,  0, 47, 79,  0, 41, 69,  8, 83,  1, 76,\n",
      "        89, 73, 78, 71, 15,  1, 53, 72, 69, 82, 69,  8, 83,  1, 78, 79,  1, 68,\n",
      "        79, 85, 66, 84,  1, 65, 66, 79, 85, 84,  1, 84, 72, 65, 84, 15,  0, 40,\n",
      "        82, 69, 71,  0, 53, 72, 65, 84,  8, 76, 76,  1, 71, 73, 86, 69,  1, 89,\n",
      "        79, 85,  1, 84, 73, 77, 69,  1, 84, 79,  1, 84, 72, 73, 78, 75,  1, 65,\n",
      "        66, 79, 85, 84,  1, 87, 72, 65, 84,  1])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "n = int(0.9 * len(corpus_str))\n",
    "\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 79, 77,  0, 40, 82, 69, 71,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([46]) the target is 79\n",
      "when input is tensor([46, 79]) the target is 77\n",
      "when input is tensor([46, 79, 77]) the target is 0\n",
      "when input is tensor([46, 79, 77,  0]) the target is 40\n",
      "when input is tensor([46, 79, 77,  0, 40]) the target is 82\n",
      "when input is tensor([46, 79, 77,  0, 40, 82]) the target is 69\n",
      "when input is tensor([46, 79, 77,  0, 40, 82, 69]) the target is 71\n",
      "when input is tensor([46, 79, 77,  0, 40, 82, 69, 71]) the target is 0\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[68,  1, 69, 65, 71, 76, 69, 83],\n",
      "        [ 1, 70, 65, 82, 84,  1, 83, 79],\n",
      "        [ 1, 84, 72, 69,  1, 52, 65, 66],\n",
      "        [83,  1, 82, 73, 68, 73, 67, 85]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 69, 65, 71, 76, 69, 83,  1],\n",
      "        [70, 65, 82, 84,  1, 83, 79, 85],\n",
      "        [84, 72, 69,  1, 52, 65, 66, 66],\n",
      "        [ 1, 82, 73, 68, 73, 67, 85, 76]])\n",
      "\n",
      "-----\n",
      "\n",
      "when input is [68] the target is 69\n",
      "when input is [68, 1] the target is 69\n",
      "when input is [68, 1, 69] the target is 69\n",
      "when input is [68, 1, 69, 65] the target is 69\n",
      "when input is [68, 1, 69, 65, 71] the target is 69\n",
      "when input is [68, 1, 69, 65, 71, 76] the target is 69\n",
      "when input is [68, 1, 69, 65, 71, 76, 69] the target is 69\n",
      "when input is [68, 1, 69, 65, 71, 76, 69, 83] the target is 69\n",
      "when input is [1] the target is 65\n",
      "when input is [1, 70] the target is 65\n",
      "when input is [1, 70, 65] the target is 65\n",
      "when input is [1, 70, 65, 82] the target is 65\n",
      "when input is [1, 70, 65, 82, 84] the target is 65\n",
      "when input is [1, 70, 65, 82, 84, 1] the target is 65\n",
      "when input is [1, 70, 65, 82, 84, 1, 83] the target is 65\n",
      "when input is [1, 70, 65, 82, 84, 1, 83, 79] the target is 65\n",
      "when input is [1] the target is 72\n",
      "when input is [1, 84] the target is 72\n",
      "when input is [1, 84, 72] the target is 72\n",
      "when input is [1, 84, 72, 69] the target is 72\n",
      "when input is [1, 84, 72, 69, 1] the target is 72\n",
      "when input is [1, 84, 72, 69, 1, 52] the target is 72\n",
      "when input is [1, 84, 72, 69, 1, 52, 65] the target is 72\n",
      "when input is [1, 84, 72, 69, 1, 52, 65, 66] the target is 72\n",
      "when input is [83] the target is 82\n",
      "when input is [83, 1] the target is 82\n",
      "when input is [83, 1, 82] the target is 82\n",
      "when input is [83, 1, 82, 73] the target is 82\n",
      "when input is [83, 1, 82, 73, 68] the target is 82\n",
      "when input is [83, 1, 82, 73, 68, 73] the target is 82\n",
      "when input is [83, 1, 82, 73, 68, 73, 67] the target is 82\n",
      "when input is [83, 1, 82, 73, 68, 73, 67, 85] the target is 82\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the max context length for predictions?\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "(\n",
    "    xb,\n",
    "    yb,\n",
    ") = get_batch(\"train\")\n",
    "\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"\\n-----\\n\")\n",
    "\n",
    "for b in range(batch_size):  # batch dim\n",
    "    for t in range(block_size):  # time dim\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, 1]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token reads the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensors of ints\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)  # (B, C, T)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # ==> (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 122])\n",
      "tensor(5.2793, grad_fn=<NllLossBackward0>)\n",
      "Expected loss: 4.804021044733257\n",
      "\n",
      "-----\n",
      "\n",
      "\n",
      "Qiíñs\\‰\"=eÉ`á*jè:]ñ[¤[,Bàuªed?ª/IAX.q[ãhô4Sk<`TAI5ÁnéKkÊYvu>1XSÁXStú‰}4<Ç=zi1tÇW+SÂHÃH}}Yñw/j#>I>w\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(\"Expected loss: \" + str(-math.log(1 / vocab_size)))\n",
    "\n",
    "print(\"\\n-----\\n\")\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "generated = decode(m.generate(idx , max_new_tokens=100)[0].tolist())\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41341495513916\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # eval the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fe b\n",
      "I'tot ish.\n",
      "Dan\n",
      "AAl the sa thth rs..\n",
      "H\n",
      "I thy t.\n",
      "Ne y ouy coovisugh\n",
      "Noyt wesnay whawr f th\n",
      "Dales tt GEte's thag ay 2 toll,120s sthe yom. fe.\n",
      "Wewe y vetowon'tho mecthoundnd thlly fine Finer gost betalle y. llies t.\n",
      "Goncowank.\n",
      "I my\n",
      "Whauat lidsn tha y ju- lonwerid cale oulan'towhtate oppacongsthr\n",
      "Bou bery mus thedit ogheroingl\n",
      "Noer, utthat\n",
      "I tesuininofo tace as az\n",
      "Oht\n",
      "RAnd t pe\n",
      "Hw tha y\n",
      "An\n",
      "Youpr arenelil\n",
      "Noit d. op y\n",
      "P.\n",
      "Whous corourcet\n",
      "Homasigout d!\n",
      "Pere fof pasele's s k! o..\n",
      "Thoit wir wo al yod\n"
     ]
    }
   ],
   "source": [
    "generated = decode(m.generate(idx , max_new_tokens=500)[0].tolist())\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
